{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [o3]- Proyecto Ozono - Predictor_v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [0] - Inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/rulicering/BigData/spark-2.4.5-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType,FloatType\n",
    "import re as reg\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#MlLib\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#Aux\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('predictor').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.0] - Carga de ficheros (Datos, Predicción clima,  Calendario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_datos = spark.read.csv('/home/rulicering/Datos_Proyecto_Ozono/Procesado/Dato_Final/Datos.csv',inferSchema= True,header=True)\n",
    "df_clima_prediccion = spark.read.csv('/home/rulicering/Datos_Proyecto_Ozono/Procesado/Clima/Clima_Prediccion-hoy.csv',inferSchema= True,header=True)\n",
    "df_calendario = spark.read.csv('/home/rulicering/Datos_Proyecto_Ozono/Procesado/Calendario/Calendario_2001-2020.csv',inferSchema= True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datos = df_datos.drop(\"_c0\")\n",
    "df_clima_prediccion = df_clima_prediccion.drop(\"_c0\")\n",
    "df_calendario = df_calendario.drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes= df_datos.columns[8:]\n",
    "magnitudes_clima = df_datos.columns[-5:]\n",
    "magnitudes_aire = df_datos.columns[8:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_clima = { \"VIENTO\":\"81\",\n",
    "                 \"DIRECCION\": \"82\",\n",
    "                 \"TEMPERATURA\": \"83\",\n",
    "                 \"PRESION\": \"87\",\n",
    "                 \"LLUVIA\":\"89\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.1] - Datos para prediccion - Prediccion clima + Calendario + Estaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ayer = (datetime.date.today() + datetime.timedelta(days = -1)).strftime(\"%Y%m%d\")\n",
    "hoy = datetime.date.today().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estaciones_aire = df_datos.filter(df_datos[\"FECHA\"]== ayer).select(\"CODIGO_CORTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_estaciones_aire = [elem[0] for elem in df_estaciones_aire.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_estaciones_aire.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hoy = df_calendario.filter(df_calendario[\"FECHA\"]== hoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calendario + Magnitudes aire a null\n",
    "for magnitud in magnitudes_aire:\n",
    "    df_hoy = df_hoy.withColumn(magnitud,F.lit(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calendario + Prediccion clima\n",
    "df_clima_hoy = df_hoy.join(df_clima_prediccion,on= \"FECHA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Estaciones cross datos clima y calendario\n",
    "df_datos_hoy = df_estaciones_aire.crossJoin(df_clima_hoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_datos_hoy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_datos_hoy.columns\n",
    "cols = cols[0:1] + cols[19:22]+ cols[1:5]+ cols[5:19] + cols[22:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datos_hoy = df_datos_hoy.select(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.1.0] - Probabilidad lluvia -> Prediccion lluvia m/l2\n",
    "    Si la probabilidad es > 50%:\n",
    "        se hace la media por estacion del historial de precipitaciones\n",
    "        cogiendo datos de +-10 días al dia de hoy de cada año anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilidad_a_lluvia_presion_ayer_aire_a_null(df_datos,df_datos_hoy):\n",
    "    mes_dia_min = (datetime.date.today() +  datetime.timedelta(days = -10)).strftime(\"%m%d\")\n",
    "    mes_dia_max = (datetime.date.today() +  datetime.timedelta(days = 10)).strftime(\"%m%d\")\n",
    "    df_historial = df_datos.filter((df_datos[\"FECHA\"]%1000 >= mes_dia_min) & (df_datos[\"FECHA\"]%1000 <= mes_dia_max))\n",
    "    df_datos_hoy = df_datos_hoy.drop('%' + dic_clima[\"LLUVIA\"])\n",
    "    l_df = []\n",
    "    for estacion in cod_estaciones_aire:\n",
    "        df_datos_hoy_estacion = df_datos_hoy.filter(df_datos_hoy[\"CODIGO_CORTO\"]==estacion)\n",
    "        aux = df_historial.filter(df_historial[\"CODIGO_CORTO\"]== estacion).select(\"FECHA\",dic_clima[\"PRESION\"],dic_clima[\"LLUVIA\"]).na.drop()\n",
    "        #Precipitacions\n",
    "        prob_lluvia_hoy = df_clima_prediccion.select(\"%\" +dic_clima[\"LLUVIA\"]).collect()[0][0]\n",
    "        prec = 0\n",
    "        if(float(prob_lluvia_hoy) > 50):\n",
    "            try:\n",
    "                prec = aux.filter(aux[dic_clima[\"LLUVIA\"]] >0).select(dic_clima[\"LLUVIA\"]).groupBy().mean().collect()[0][0]\n",
    "            except:\n",
    "                print(\"[WARN]: No hay lluvias historicas en ese rango de fechas\")\n",
    "        df_datos_hoy_estacion = df_datos_hoy_estacion.withColumn(dic_clima[\"LLUVIA\"],F.lit(prec))\n",
    "            \n",
    "        #Presion\n",
    "        ayer = (datetime.date.today() + datetime.timedelta(days= -1)).strftime(\"%Y%m%d\")\n",
    "        presion_ayer = aux.filter(aux[\"FECHA\"]==ayer).select(dic_clima[\"PRESION\"]).collect()[0][0]\n",
    "        df_datos_hoy_estacion = df_datos_hoy_estacion.withColumn(dic_clima[\"PRESION\"],F.lit(presion_ayer))\n",
    "\n",
    "        l_df.append(df_datos_hoy_estacion)\n",
    "        \n",
    "    df_datos_hoy = l_df[0]\n",
    "    for i in range(1,len(l_df)):\n",
    "        df_datos_hoy = df_datos_hoy.union(l_df[i])\n",
    "    return df_datos_hoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_datos_hoy = probabilidad_a_lluvia_presion_ayer_aire_a_null(df_datos,df_datos_hoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_datos_hoy.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.2] - Union Datos + Datos hoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_datos= df_datos.union(df_datos_hoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.3] - Dar cada fila de datos +  contaminacion ayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventana = Window.partitionBy(\"CODIGO_CORTO\").orderBy(\"FECHA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for magnitud in magnitudes_aire:\n",
    "    df_datos = df_datos.withColumn(\"A_%s\"%magnitud, F.lag(magnitud,1,None).over(ventana))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_datos.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.4] - Tipos\n",
    "    \n",
    "    CODIGO_CORTO -> INTEGER\n",
    "    ANO -> INTEGER\n",
    "    MES -> INTEGER\n",
    "    DIA -> INTEGER\n",
    "    FECHA -> INTEGER\n",
    "    DIASEMANA -> INTEGER\n",
    "    TIPODIA -> INTEGER\n",
    "    CONFINAMIENTO -> INTEGER\n",
    "    MEDICIONES -> DOUBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_datos.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] - PREDICCIONES\n",
    "    \n",
    "    Se hace 1 a 1 para cada magnitud de contaminación\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_comunes = df_datos.columns[0:8] + magnitudes_clima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.0] - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for magnitud in magnitudes_aire:\n",
    "    print(\"=\"*20, magnitud, \"=\"*20)\n",
    "    cols_comunes = df_datos.columns[0:8] + magnitudes_clima\n",
    "    cols_features = cols_comunes + [\"A_%s\"%magnitud]\n",
    "    assembler = VectorAssembler(inputCols = cols_features, outputCol = \"F_%s\" % magnitud)\n",
    "    #Limpiamos las filas con el dato para esa magnitud a Null\n",
    "    cols_y_magnitud = cols_features + [magnitud]\n",
    "    df_datos_magnitud = df_datos.select(cols_y_magnitud).na.drop()\n",
    "    #Mirar a ver que hacemos cuando el clima es null\n",
    "    \n",
    "    output = assembler.transform(df_datos_magnitud)\n",
    "    #output.printSchema()\n",
    "    #Ver cómo funciona\n",
    "    final_data = output.filter(output[\"FECHA\"] < hoy).select(\"F_%s\" %magnitud, magnitud)\n",
    "    training_data,test_data = final_data.randomSplit([0.9,0.1])\n",
    "    \n",
    "    #train_data = output.filter(output[\"FECHA\"] < hoy).select(\"F_%s\" %magnitud, magnitud)\n",
    "    #test_data = output.filter(output[\"FECHA\"] == hoy).select(\"F_%s\" %magnitud, magnitud)\n",
    "    \n",
    "    lr = LinearRegression(featuresCol =\"F_%s\" %magnitud, labelCol = magnitud)\n",
    "    lr_model = lr.fit(training_data)\n",
    "    test_results = lr_model.evaluate(test_data)\n",
    "    #test_results.residuals.show()\n",
    "    print(\"Root mean Squared Error: \",test_results.rootMeanSquaredError)\n",
    "    print(\"R2: \" ,test_results.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.1] - GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 1 ====================\n",
      "+------------------+----+--------------------+\n",
      "|        prediction|   1|                 F_1|\n",
      "+------------------+----+--------------------+\n",
      "| 5.695896504060072|null|[40.0,2020.0,5.0,...|\n",
      "| 4.728451325660754|null|[57.0,2020.0,5.0,...|\n",
      "| 7.619320620244932|null|[17.0,2020.0,5.0,...|\n",
      "| 4.788778655051824|null|[35.0,2020.0,5.0,...|\n",
      "| 4.846021486922737|null|[4.0,2020.0,5.0,1...|\n",
      "|6.5678524261741895|null|[8.0,2020.0,5.0,1...|\n",
      "|2.6807944794058063|null|[38.0,2020.0,5.0,...|\n",
      "| 4.751049148079721|null|[24.0,2020.0,5.0,...|\n",
      "| 5.729458350188311|null|[18.0,2020.0,5.0,...|\n",
      "| 4.761360888490631|null|[36.0,2020.0,5.0,...|\n",
      "+------------------+----+--------------------+\n",
      "\n",
      "==================== 10 ====================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-13ab59338ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#Creamos el indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mfeatureIndexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"F_%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmagnitud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"indexedFeatures\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxCategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m#Partimos el dato\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtrainingData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"FECHA\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mhoy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F_%s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mmagnitud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnitud\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/BigData/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/BigData/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/BigData/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l_predicciones = []\n",
    "for magnitud in magnitudes_aire:\n",
    "    print(\"=\"*20, magnitud, \"=\"*20)\n",
    "    cols_comunes = df_datos.columns[0:8] + magnitudes_clima\n",
    "    cols_features = cols_comunes + [\"A_%s\"%magnitud]\n",
    "\n",
    "    #Limpiamos las filas con el dato para esa magnitud a Null\n",
    "    cols_y_magnitud = cols_features + [magnitud]\n",
    "    df_datos_magnitud_calculados = df_datos.filter(df_datos[\"FECHA\"] <hoy).select(cols_y_magnitud).na.drop()\n",
    "    df_datos_magnitud_hoy = df_datos.filter(df_datos[\"FECHA\"] == hoy).select(cols_y_magnitud).na.drop(subset = \"A_%s\"%magnitud)\n",
    "    df_datos_magnitud = df_datos_magnitud_calculados.union(df_datos_magnitud_hoy)\n",
    "    \n",
    "    #Assembles to create features column\n",
    "    assembler = VectorAssembler(inputCols = cols_features, outputCol = \"F_%s\" % magnitud)\n",
    "    data_assembled = assembler.transform(df_datos_magnitud)\n",
    "    \n",
    "    #Seleccionamos las filas que vamos a utilizar\n",
    "    data = data_assembled.select(\"FECHA\",\"F_%s\" %magnitud, magnitud)\n",
    "\n",
    "    #Creamos el indexer\n",
    "    featureIndexer = VectorIndexer(inputCol=\"F_%s\" % magnitud, outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "    #Partimos el dato\n",
    "    trainingData = data.filter(data[\"FECHA\"]< hoy).select(\"F_%s\" %magnitud, magnitud)\n",
    "    datatopredict = data.filter(data[\"FECHA\"] == hoy).select(\"F_%s\" %magnitud, magnitud)\n",
    "\n",
    "    # Train a GBT model.\n",
    "    gbt = GBTRegressor(featuresCol=\"indexedFeatures\", labelCol=magnitud,maxIter=20)\n",
    "    \n",
    "    #Pipeline\n",
    "    pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "    \n",
    "    # Train model.  This also runs the indexer.\n",
    "    model = pipeline.fit(trainingData)\n",
    "    \n",
    "    # Make predictions.\n",
    "    predictions = model.transform(datatopredict)\n",
    "    \n",
    "    # Select example rows to display\n",
    "    predictions.select(\"prediction\", magnitud , \"F_%s\" % magnitud).show()\n",
    "    l_predicciones.append(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#Para evaluar\n",
    "for magnitud in magnitudes_aire:\n",
    "    print(\"=\"*20, magnitud, \"=\"*20)\n",
    "    cols_comunes = df_datos.columns[0:8] + magnitudes_clima\n",
    "    cols_features = cols_comunes + [\"A_%s\"%magnitud]\n",
    "    \n",
    "    #Limpiamos las filas con el dato para esa magnitud a Null\n",
    "    cols_y_magnitud = cols_features + [magnitud]\n",
    "    df_datos_magnitud = df_datos.select(cols_y_magnitud).na.drop()\n",
    "      \n",
    "    #Assembles to create features column\n",
    "    assembler = VectorAssembler(inputCols = cols_features, outputCol = \"F_%s\" % magnitud)\n",
    "    data_assembled = assembler.transform(df_datos_magnitud)\n",
    "    \n",
    "    #Seleccionamos las filas que vamos a utilizar\n",
    "    data = data_assembled.filter(data_assembled[\"FECHA\"] < hoy).select(\"F_%s\" %magnitud, magnitud)\n",
    "    #Creamos el indexer\n",
    "    featureIndexer = VectorIndexer(inputCol=\"F_%s\" % magnitud, outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "    #Partimos el dato\n",
    "    (trainingData,testData) = data.randomSplit([0.9,0.1])\n",
    "    #(trainingData,testData) = data.randomSplit([0.7,0.3])\n",
    "    # Train a GBT model.\n",
    "    gbt = GBTRegressor(featuresCol=\"indexedFeatures\", labelCol=magnitud,maxIter=20)\n",
    "    \n",
    "    #Pipeline\n",
    "    pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "    \n",
    "    # Train model.  This also runs the indexer.\n",
    "    model = pipeline.fit(trainingData)\n",
    "    \n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "    \n",
    "    # Select example rows to display\n",
    "    predictions.select(\"prediction\", magnitud , \"F_%s\" % magnitud).show(5)\n",
    "    \n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=magnitud, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "    print(\"R2 on test data = %g\" % r2)\n",
    "    \n",
    "    gbtModel = model.stages[1]\n",
    "    print(gbtModel) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4] - FORMATO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Borramos las columnas utilizadas para relacionar estaciones de aire y clima\n",
    "a_borrar = c_grupo_14 + c_estacionxmagnitud_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd_final = pd_datos_y_calendario.drop(columns = a_borrar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [5] - EXPORTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Versiones\n",
    "hoy = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "pd_final.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Dato_Final/Datos_2014-NOW-\" + hoy + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_final.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Dato_Final/Datos_2014-NOW.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [EXTRA] - CHECKEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_chequeo = pd_datos[['CODIGO_CORTO', 'FECHA','E_AEMET','E_81', 'E_82', 'E_83', 'E_87', 'E_89', '81', '82', '83', '87', '89']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2014-2018\n",
    "#pd_chequeo[(pd_datos[\"FECHA\"]==20140101)].sort_values(by=\"E_AEMET\")\n",
    "#pd_clima[pd_clima[\"FECHA\"]==20140101]\n",
    "\n",
    "#2019-NOW\n",
    "#pd_chequeo[(pd_datos[\"FECHA\"]==20190101)]\n",
    "#pd_clima[pd_clima[\"FECHA\"]==20190101]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
