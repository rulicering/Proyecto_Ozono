{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [o3]- Proyecto Ozono - Predictor_v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [0] - Inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/rulicering/BigData/spark-2.4.5-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType,FloatType\n",
    "import re as reg\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('predictor').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.0] - Carga de ficheros (Datos, Predicción clima,  Calendario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_datos = spark.read.csv('/home/rulicering/Datos_Proyecto_Ozono/Procesado/Dato_Final/Datos.csv',inferSchema= True,header=True)\n",
    "df_clima_prediccion = spark.read.csv('/home/rulicering/Datos_Proyecto_Ozono/Procesado/Clima/Clima_Prediccion-hoy.csv',inferSchema= True,header=True)\n",
    "df_calendario = spark.read.csv('/home/rulicering/Datos_Proyecto_Ozono/Procesado/Calendario/Calendario_2013-2020.csv',inferSchema= True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datos = df_datos.drop(\"_c0\")\n",
    "df_clima_prediccion = df_clima_prediccion.drop(\"_c0\")\n",
    "df_calendario = df_calendario.drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.1] - Dar cada fila de datos + datos clima y contaminacion ayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [elem[0] for elem in df_datos.select(\"1\").collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista.pop()\n",
    "lista.insert(0,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[elem[0] for elem in df_datos.select(\"1\").collect()].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = [0,1,2,3,4,5,6,7]\n",
    "lista.pop()\n",
    "lista.insert(0,None)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_funct(columna):\n",
    "    lista = [elem[0] for elem in df_datos.select(\"1\").collect()]\n",
    "    lista.pop()\n",
    "    lista.insert(0,None)\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/45011320/how-to-get-data-of-previous-row-in-apache-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-1ae4d88e6413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_datos_con_dia_anterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_datos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A_1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmy_funct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_datos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/BigData/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m         \"\"\"\n\u001b[0;32m-> 1996\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1997\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "df_datos_con_dia_anterior = df_datos.withColumn(\"A_1\",my_funct(df_datos.select(\"1\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CODIGO_CORTO',\n",
       " 'ANO',\n",
       " 'MES',\n",
       " 'DIA',\n",
       " 'FECHA',\n",
       " '1',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '12',\n",
       " '14',\n",
       " '20',\n",
       " '30',\n",
       " '35',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '87',\n",
       " '89',\n",
       " 'DIASEMANA',\n",
       " 'TIPODIA',\n",
       " 'CONFINAMIENTO']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_datos.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.1] - Estaciones aire + prediccion clima + calendario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes_clima = df_clima_prediccion.columns[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ayer = (datetime.date.today() - datetime.timedelta(days=-2)).strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datos_ayer = df_datos.filter(df_datos[\"FECHA\"]==20200101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_datos_ayer = df_datos_ayer.drop(\"ANO\",\"MES\",\"DIA\",\"FECHA\",\"DIASEMANA\",\"TIPO_DIA\",\"CONFINAMIENTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for magnitud in magnitudes_clima:\n",
    "    df_datos_ayer = df_datos_ayer.withColumn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.1] - Paso a Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd_aire = df_aire.drop(\"_c0\").toPandas()\n",
    "pd_clima = df_clima.drop(\"_c0\").toPandas()\n",
    "pd_estaciones = df_estaciones.drop(\"_c0\").toPandas()\n",
    "pd_calendario = df_calendario.drop(\"_c0\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.2] - Tipos\n",
    "    \n",
    "    CODIGO_CORTO -> STRING/OBJECT\n",
    "    ANO -> INTEGER\n",
    "    MES -> INTEGER\n",
    "    DIA -> INTEGER\n",
    "    FECHA -> INTEGER\n",
    "    MEDICIONES -> DOUBLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.2.0] -  Estaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#None to NULO\n",
    "pd_estaciones = pd_estaciones.replace(('None',None),np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E_AEMET']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Columnas estaciones medicion - 2014\n",
    "regex = reg.compile(\"E_AEMET\")\n",
    "c_grupo_14 = [elem for elem in list(filter(regex.search,df_estaciones.columns))]\n",
    "c_grupo_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E_81', 'E_82', 'E_83', 'E_87', 'E_89']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Columnas estaciones medicion - 2019\n",
    "regex = reg.compile(\"E_\\d\\d\")\n",
    "c_estacionxmagnitud_19 = [elem for elem in list(filter(regex.search,df_estaciones.columns))]\n",
    "c_estacionxmagnitud_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columnas estaciones medicion a string\n",
    "for columna in c_estacionxmagnitud_19:\n",
    "    pd_estaciones[columna] = pd_estaciones[columna].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_estaciones.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [1.2.1] - Aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#None to NULO\n",
    "pd_aire = pd_aire.replace(('None',None),np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODIGOS CORTOS A Strig\n",
    "pd_aire[\"CODIGO_CORTO\"] = pd_aire[\"CODIGO_CORTO\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_aire = pd_aire.sort_values(by=\"FECHA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pd_aire.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.2.2] - Clima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#None to NULO\n",
    "pd_clima = pd_clima.replace(('None',None),np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Listar las columnas que miden magnitudes\n",
    "columnas_valoresmagnitudes = list(pd_clima.columns)[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columnas magnitudes a float\n",
    "for columna in columnas_valoresmagnitudes:\n",
    "    #Comas por puntos\n",
    "    pd_clima[columna]  =  [reg.sub(',','.',str(x)) for x in pd_clima[columna]]\n",
    "    # String to float\n",
    "    pd_clima[columna] = pd_clima[columna].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_clima = pd_clima.sort_values(by=\"FECHA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pd_clima.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.2.3] - Calendario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_calendario = pd_calendario.replace(('None',None),np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.3] - Darle grupo a estaciones de AIRE\n",
    "\n",
    "    2014-2018 -> E_AEMET\n",
    "    \n",
    "    2019-NOW -> 81,82,83,87,89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacamos estaciones de aire y codigos de estaciones de clima asociadas\n",
    "pd_estaciones_aire = pd_estaciones[pd_estaciones[\"MIDE_AIRE\"]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nos quedamos con las columnas que queremos\n",
    "c_agrupamiento = [\"CODIGO_CORTO\"] + c_grupo_14 + c_estacionxmagnitud_19\n",
    "pd_estaciones_aire = pd_estaciones_aire[c_agrupamiento]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unimos ambos datasets\n",
    "pd_aire = pd_aire.merge(pd_estaciones_aire, on =[\"CODIGO_CORTO\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] -  UNIR AIRE + CLIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.0] - Separamos datos contaminacion (2014-18 | 2019-NOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos 14-18\n",
    "pd_datos_14_18 = pd_aire[pd_aire[\"FECHA\"]<20190101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos 19-NOW\n",
    "pd_datos_19_NOW = pd_aire[pd_aire[\"FECHA\"]>=20190101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.1] - MERGE (ESTACION AIRE, DIA) + (DATOS CLIMA )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos columnas de info (comunes) y las columnas de datos\n",
    "columnas = list(pd_clima.columns) \n",
    "c_info = columnas[:5]\n",
    "c_magnitudes = columnas[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Asociamos cada columna de datos de clima (magnitud) a la estación de aire correspondiente.  \n",
    "for magnitud in c_magnitudes:\n",
    "    cols = c_info.copy()\n",
    "    cols.append(magnitud)\n",
    "    \n",
    "    pd_clima_magnitud = pd_clima[cols]\n",
    "\n",
    "    #2014-2018\n",
    "    pd_clima_magnitud_14_18 = pd_clima_magnitud.rename(columns={\"CODIGO_CORTO\":c_grupo_14[0]})\n",
    "    pd_datos_14_18 = pd_datos_14_18.merge(pd_clima_magnitud_14_18,on = [\"ANO\", \"MES\", \"DIA\",\"FECHA\",c_grupo_14[0]])\n",
    "    \n",
    "    #2019-NOW\n",
    "    pd_clima_magnitud_19_NOW = pd_clima_magnitud.rename(columns={\"CODIGO_CORTO\":\"E_%s\"%magnitud})\n",
    "    pd_datos_19_NOW = pd_datos_19_NOW.merge(pd_clima_magnitud_19_NOW,on = [\"ANO\", \"MES\", \"DIA\",\"FECHA\",\"E_%s\"%magnitud])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.2] - Unimos partes generadas por el MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd_datos_14_18.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_datos_19_NOW.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd_datos = pd.concat([pd_datos_14_18,pd_datos_19_NOW])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] - Incluir CALENDARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd_datos_y_calendario = pd_datos.merge(pd_calendario, on = \"FECHA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_datos_y_calendario.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4] - FORMATO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Borramos las columnas utilizadas para relacionar estaciones de aire y clima\n",
    "a_borrar = c_grupo_14 + c_estacionxmagnitud_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd_final = pd_datos_y_calendario.drop(columns = a_borrar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [5] - EXPORTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Versiones\n",
    "hoy = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "pd_final.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Dato_Final/Datos_2014-NOW-\" + hoy + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_final.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Dato_Final/Datos_2014-NOW.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [EXTRA] - CHECKEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_chequeo = pd_datos[['CODIGO_CORTO', 'FECHA','E_AEMET','E_81', 'E_82', 'E_83', 'E_87', 'E_89', '81', '82', '83', '87', '89']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2014-2018\n",
    "#pd_chequeo[(pd_datos[\"FECHA\"]==20140101)].sort_values(by=\"E_AEMET\")\n",
    "#pd_clima[pd_clima[\"FECHA\"]==20140101]\n",
    "\n",
    "#2019-NOW\n",
    "#pd_chequeo[(pd_datos[\"FECHA\"]==20190101)]\n",
    "#pd_clima[pd_clima[\"FECHA\"]==20190101]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
