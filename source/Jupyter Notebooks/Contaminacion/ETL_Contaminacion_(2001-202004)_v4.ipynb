{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [o3]- Proyecto Ozono - ETL_Contaminación (2001-202004) - v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    0. Inicializacion\n",
    "    1. Datos\n",
    "        1.0 Carga ficheros AIRE ( 2001-2020) - URL\n",
    "        1.1 FORMATO:  [ESTACION,MAGNITUD,ANO,MES,DATO_DIA1,VALIDO,DATO_DIA2,VALIDO...]\n",
    "        1.2 Creacion nuevo esquema\n",
    "        1.3 Creacion nuevo DF vacio\n",
    "        1.4 Nuevo DF -> [ESTACION,MAGNITUD,ANO,MES,DIA,FECHA,VALOR,VALIDO]\n",
    "        1.5 Columna FECHA -> 20140101\n",
    "        1.6 Colocar columnas -> [ESTACION,MAGNITUD, ANO,MES,DIA, FECHA, VALOR,VALIDO]\n",
    "        1.7 VALOR ? VALIDO -> VALOR VALIDADO\n",
    "        1.8 PIVOT Magnitudes\n",
    "        1.9 Limpiar: Meses 31 días + Meses 30 días + Febreros\n",
    "    2. Formato\n",
    "    3. Ordenar\n",
    "    4. Exportar\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [0] - Inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import findspark\n",
    "findspark.init('/home/rulicering/BigData/spark-2.4.5-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField,StringType,IntegerType,FloatType,StructType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('contaminacion').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] - Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.0] - Carga ficheros AIRE ( 2014-2020) - URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anos= (2001,2020)\n",
    "urls =[\"https://datos.madrid.es/egob/catalogo/201410-10306604-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306602-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306600-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306598-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306596-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306594-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306592-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306590-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306588-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306586-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306584-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306582-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306580-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306578-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306576-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306574-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-7775098-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-7775096-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306606-calidad-aire-diario.csv\",\n",
    "       \"https://datos.madrid.es/egob/catalogo/201410-10306609-calidad-aire-diario.csv\"  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_read_to_df(url):\n",
    "    pdf = pd.read_csv(url,sep=';')\n",
    "    df = spark.createDataFrame(pdf)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes_aire = [\"1\",\"6\",\"7\",\"8\",\"9\",\"10\",\"12\",\"14\",\"20\",\"30\",\"35\",\"42\",\"43\",\"44\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesoxano(anos):\n",
    "    l_pd = []\n",
    "    for i in range(anos[1] - anos[0]+1):\n",
    "        print(anos[0]+i)\n",
    "        df = pd_read_to_df(urls[i])\n",
    "        # [1.1] - FORMATO:  [ESTACION,MAGNITUD,ANO,MES,DATO_DIA1,VALIDO,DATO_DIA2,VALIDO...]\n",
    "        #Eliminamos columnas no esenciales\n",
    "        df = df.select('ESTACION','MAGNITUD','ANO','MES','D01','V01','D02','V02','D03','V03','D04','V04','D05','V05','D06','V06','D07','V07','D08','V08','D09','V09','D10','V10','D11','V11','D12','V12','D13','V13','D14','V14','D15','V15','D16','V16','D17','V17','D18','V18','D19','V19','D20','V20','D21','V21','D22','V22','D23','V23','D24','V24','D25','V25','D26','V26','D27','V27','D28','V28','D29','V29','D30','V30','D31','V31')\n",
    "        # [1.2] - Creacion nuevo esquema\n",
    "        data_schema = [StructField('ESTACION',IntegerType(), False), \n",
    "              StructField('MAGNITUD',IntegerType(), False),\n",
    "              StructField('ANO',IntegerType(), False),\n",
    "              StructField('MES',IntegerType(), False),\n",
    "              StructField('VALOR',FloatType(), True),\n",
    "              StructField('VALIDO',IntegerType(), False),\n",
    "              StructField('DIA',IntegerType(), False)]\n",
    "        struct = StructType(fields = data_schema)\n",
    "        # [1.3] - Creacion nuevo DF vacio\n",
    "        df_v1 = spark.createDataFrame(spark.sparkContext.emptyRDD(),struct)\n",
    "        # [1.4] - Nuevo DF -> [ESTACION,MAGNITUD,ANO,MES,DIA,FECHA,VALOR,VALIDO]\n",
    "        for i in range(1,32): #Días  \n",
    "            valor = 'D%02d' % i\n",
    "            valido = 'V%02d' % i\n",
    "            df_v1 = df_v1.union(df.select(\"ESTACION\",\"MAGNITUD\",\"ANO\",\"MES\",valor,valido).withColumn('DIA', F.lit(i)))\n",
    "\n",
    "        df = df_v1\n",
    "        # [1.5] - Columna FECHA -> 20140101\n",
    "        df = df.withColumn(\"FECHA\",df[\"ANO\"]*10000 + df[\"MES\"]*100 + df[\"DIA\"])\n",
    "        # [1.6] -Colocar columnas -> [ESTACION, MAGNITUD, ANO,MES,DIA, FECHA, VALOR,VALIDO]\n",
    "        cols = df.columns\n",
    "        cols = cols[:4] + cols[-2:] + cols[-4:-2]\n",
    "        df= df[cols]\n",
    "        # [1.7] - VALOR ? VALIDO -> VALOR VALIDADO\n",
    "        df = df.withColumn(\"VALOR VALIDADO\",F.when(F.col(\"VALIDO\")== 'N',None).otherwise(F.col(\"VALOR\")) )\n",
    "        df = df.select(\"ESTACION\",\"MAGNITUD\",\"ANO\",\"MES\",\"DIA\",\"FECHA\",df[\"VALOR VALIDADO\"].alias(\"VALOR\"))\n",
    "        ## [1.8] - PIVOT Magnitudes\n",
    "        df = df.groupBy('ESTACION','ANO', 'MES', 'DIA',\"FECHA\").pivot(\"MAGNITUD\").sum(\"VALOR\").orderBy(\"FECHA\")\n",
    "        ## [1.9] - Limpiar: Meses 31 días + Meses 30 días + Febreros\n",
    "        #Esto se puede hacer en el paso 6\n",
    "        df_31 = df.filter(df[\"MES\"].isin([1,3,5,7,8,10,12]))\n",
    "        df_30 = df.filter((df[\"MES\"].isin([4,6,9,11])) & (df[\"DIA\"] <31))\n",
    "        df_feb = df.filter((df[\"MES\"] == 2) & (df[\"DIA\"] <30)) #Para no excluir los bisiestos\n",
    "        df = df_31.union(df_30).union(df_feb)\n",
    "        # [2] - Formato\n",
    "        df = df.withColumn(\"ESTACION\",df[\"ESTACION\"].cast(StringType()))\n",
    "        df = df.withColumnRenamed(\"ESTACION\",\"CODIGO_CORTO\")\n",
    "        #Estandarizar el numero de columnas. Las que no existen, a nulo\n",
    "        cols_magnitudes = df.columns[5:]\n",
    "        for magnitud in magnitudes_aire:\n",
    "            if(magnitud not in cols_magnitudes):\n",
    "                df = df.withColumn(magnitud,F.lit(None))\n",
    "        #Ordenamos las columnas\n",
    "        cols_magnitudes = df.columns[5:]\n",
    "        cols_magnitudes.sort()\n",
    "        cols = df.columns[0:5] + cols_magnitudes\n",
    "        df = df.select(cols)\n",
    "        #Lo pasamos a pandas y lo añadimos a la lista (Por motivos de capacidad computacional)\n",
    "        pd_df = df.toPandas()\n",
    "        l_pd.append(pd_df)\n",
    "\n",
    "    #Unimos los dfs de cada año\n",
    "    pd_final = pd.concat(l_pd)\n",
    "    return pd_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "pd = procesoxano(anos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] - Ordenar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sort(\"FECHA\",ascending= True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4] - Exportar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_final = pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Versiones\n",
    "hoy = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "pd_final.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Contaminacion/BackUp/Contaminacion_diaria_2001-202004-\" + hoy +\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_final.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Contaminacion/Contaminacion_diaria_2001-202004.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
