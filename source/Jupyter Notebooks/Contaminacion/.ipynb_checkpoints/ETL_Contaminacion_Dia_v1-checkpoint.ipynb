{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [o3]- Proyecto Ozono - ETL_Contaminación_Dia_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    0. Inicializacion\n",
    "    1. Datos\n",
    "        1.0 Carga ficheros AIRE ( Tiempo Real) - URL\n",
    "        1.1 FORMATO:\n",
    "            [ESTACION,MAGNITUD,ANO,MES,DIA,DATO_HORA1,VALIDO,DATO_HORA2,VALIDO.]\n",
    "        1.2 Creacion nuevo esquema\n",
    "        1.3 Creacion nuevo DF vacio\n",
    "        1.4 Nuevo DF -> [ESTACION,MAGNITUD,ANO,MES,DIA,HORA,VALOR,VALIDO]\n",
    "        1.5 Columna FECHA -> 20140101\n",
    "        1.6 Colocar columnas -> \n",
    "            [ESTACION, MAGNITUD, ANO,MES,DIA,HORA,FECHA,VALOR,VALIDO]\n",
    "        1.7 VALOR ? VALIDO -> VALOR VALIDADO\n",
    "        1.8 PIVOT Magnitudes\n",
    "    2. Formato\n",
    "    3. Ordenar\n",
    "    4. Media diaria\n",
    "    5. Exportar\n",
    "        5.0 Dia x Horas\n",
    "        5.1 Media dia\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [0] - Inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import findspark\n",
    "findspark.init('/home/rulicering/BigData/spark-2.4.5-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField,StringType,IntegerType,FloatType,StructType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('contaminacion_hoy').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] - Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.0] - Carga ficheros AIRE ( Tiempo Real) - URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  \"http://www.mambiente.madrid.es/opendata/horario.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr = {\"Host\": \"www.mambiente.madrid.es\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:75.0) Gecko/20100101 Firefox/75.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cookie\": \"MadridCookiesPolicy=; _hjid=0faf8542-e813-4294-b619-5eb313249e16\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url = url, headers = hdr)\n",
    "#r2.headers\n",
    "file_object = io.StringIO(r.content.decode(r.apparent_encoding))\n",
    "pd_aire_hoy = pd.read_csv(file_object,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pd_aire_hoy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd_aire_hoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.1] - FORMATO: \n",
    "### [ESTACION,MAGNITUD,ANO,MES,DIA,DATO_HORA1,VALIDO,DATO_HORA2,VALIDO...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos columnas no esenciales\n",
    "df = df.select('ESTACION','MAGNITUD','ANO','MES','DIA','H01','V01','H02','V02','H03','V03','H04','V04','H05','V05','H06','V06','H07','V07','H08','V08','H09','V09','H10','V10','H11','V11','H12','V12','H13','V13','H14','V14','H15','V15','H16','V16','H17','V17','H18','V18','H19','V19','H20','V20','H21','V21','H22','V22','H23','V23','H24','V24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.2] - Creacion nuevo esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField('ESTACION',IntegerType(), False), \n",
    "              StructField('MAGNITUD',IntegerType(), False),\n",
    "              StructField('ANO',IntegerType(), False),\n",
    "              StructField('MES',IntegerType(), False),\n",
    "              StructField('DIA',IntegerType(), False),\n",
    "              StructField('VALOR',FloatType(), True),\n",
    "              StructField('VALIDO',IntegerType(), False),\n",
    "              StructField('HORA',IntegerType(), False)]\n",
    "struct = StructType(fields = data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.3] - Creacion nuevo DF vacio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v1 = spark.createDataFrame(spark.sparkContext.emptyRDD(),struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.4] - Nuevo DF -> [ESTACION,MAGNITUD,ANO,MES,DIA,HORA,VALOR,VALIDO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,25): #Horas dia\n",
    "    valor = 'H%02d' % i\n",
    "    valido = 'V%02d' % i\n",
    "    df_v1 = df_v1.union(df.select(\"ESTACION\",\"MAGNITUD\",\"ANO\",\"MES\",\"DIA\",valor,valido).withColumn('HORA', F.lit(i)))\n",
    "\n",
    "df = df_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.5] - Columna FECHA -> 20140101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"FECHA\",df[\"ANO\"]*10000 + df[\"MES\"]*100 + df[\"DIA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.6] -Colocar columnas \n",
    "### [ESTACION, MAGNITUD, ANO,MES,DIA, HORA,FECHA, VALOR,VALIDO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colocar las columnas\n",
    "cols = df.columns\n",
    "cols = cols[:5] + cols[-2:] + cols[-4:-2]\n",
    "df= df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.7] - VALOR ? VALIDO -> VALOR VALIDADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"VALOR VALIDADO\",F.when(F.col(\"VALIDO\")== 'N',None).otherwise(F.col(\"VALOR\")) )\n",
    "df = df.select(\"ESTACION\",\"MAGNITUD\",\"ANO\",\"MES\",\"DIA\",\"HORA\",\"FECHA\",df[\"VALOR VALIDADO\"].alias(\"VALOR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.8] - PIVOT Magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy('ESTACION','ANO', 'MES', 'DIA','HORA',\"FECHA\").pivot(\"MAGNITUD\").sum(\"VALOR\").orderBy(\"FECHA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] - Formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"ESTACION\",df[\"ESTACION\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = df.toPandas()\n",
    "pd = pd.rename(columns={\"ESTACION\":\"CODIGO_CORTO\"})\n",
    "pd_diaxhoras = pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] - Ordenar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_magnitudes = pd_diaxhoras.columns.tolist()[6:]\n",
    "cols_magnitudes.sort()\n",
    "cols = pd_diaxhoras.columns.tolist()[0:6] + cols_magnitudes\n",
    "pd_diaxhoras = pd_diaxhoras[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_diaxhoras = pd_diaxhoras.sort_values(by=[\"ANO\",\"MES\",\"DIA\",\"FECHA\",\"HORA\",\"CODIGO_CORTO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_diaxhoras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4] - Media diaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_media_dia = pd_diaxhoras.groupby(by=[\"ANO\",\"MES\",\"DIA\",\"FECHA\",\"CODIGO_CORTO\"]).agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Quitamos la columna \"HORA\"\n",
    "cols = pd_media_dia.columns\n",
    "cols = cols[1:] \n",
    "pd_media_dia = pd_media_dia[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_media_dia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [5] - Exportar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoy = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "ayer = (datetime.date.today() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5.0] - Dia x Horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_diaxhoras.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BackUp\n",
    "pd_diaxhoras.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Contaminacion/BackUp/Contaminacion_diaxhoras-\" + hoy +\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Contaminacion_diaxhoras- 2020-05-21 .csv --- Created successfully\n"
     ]
    }
   ],
   "source": [
    "pd_diaxhoras.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Contaminacion/Contaminacion_diaxhoras-\"+ hoy +\".csv\")\n",
    "print(\"[INFO] - Contaminacion_diaxhoras-\"+ hoy +\".csv --- Created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] - Contaminacion_diaxhoras- 2020-05-20 .csv --- Could not been removed\n"
     ]
    }
   ],
   "source": [
    "#Borrar la de ayer\n",
    "try:\n",
    "    os.remove(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Contaminacion/Contaminacion_diaxhoras-\"+ ayer +\".csv\")\n",
    "    print(\"[INFO] - Contaminacion_diaxhoras-\"+  ayer+\".csv --- Removed successfully\")\n",
    "except:\n",
    "    print(\"[ERROR] - Contaminacion_diaxhoras-\"+ ayer+\".csv --- Could not been removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5.1] -Media dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd_media_dia.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BackUp\n",
    "pd_media_dia.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Contaminacion/BackUp/Contaminacion_mediadia-\" + hoy +\".csv\",float_format = '%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Contaminacion_mediadia- 2020-05-21 .csv --- Created successfully\n"
     ]
    }
   ],
   "source": [
    "pd_media_dia.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Contaminacion/Contaminacion_mediadia-\" + hoy +\".csv\",float_format = '%.8f')\n",
    "print(\"[INFO] - Contaminacion_mediadia-\"+ hoy +\".csv --- Created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Borrar la de ayer\n",
    "try:\n",
    "    os.remove(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Contaminacion/Contaminacion_mediadia-\" + ayer +\".csv\")\n",
    "    print(\"[INFO] - Contaminacion_mediadia-\"+ ayer+\".csv --- Removed successfully\")\n",
    "except:\n",
    "    print(\"[ERROR] - Contaminacion_mediadia-\"+ ayer+\".csv --- Could not been removed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
