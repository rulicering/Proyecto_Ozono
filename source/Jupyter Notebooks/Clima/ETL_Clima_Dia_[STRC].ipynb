{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [o3] - Proyecto Ozono - ETL_Clima_Dia_[STRC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import swagger_client\n",
    "from swagger_client.rest import ApiException\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import findspark\n",
    "findspark.init('/home/rulicering/BigData/spark-2.4.5-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import requests\n",
    "import numpy as np\n",
    "import re as reg\n",
    "import json\n",
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType,FloatType\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimaDia():\n",
    "    \"\"\"\n",
    "    \n",
    "        FUNCIONES AUXILIARES<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def data_to_sparkdf(self,data):\n",
    "        #Encoding \"ISO-8859\"\n",
    "        data_v = data.decode(encoding ='ISO-8859-15')\n",
    "        data_v0 = data_v\n",
    "\n",
    "        # Clean the data\n",
    "        for i in range(20):\n",
    "            if(data_v0[i]=='{'):\n",
    "                data_v0 = data_v0[i:]\n",
    "        for i in range(20):\n",
    "            if(data_v0[-i]=='}'):\n",
    "                data_v0 = data_v0[:-i+1]\n",
    "\n",
    "        data_v1 = data_v0\n",
    "        data_v1 = data_v1.replace(\"\\n\", \"\")\n",
    "\n",
    "        data_v2 = data_v1.replace(\"},\",\"}};\")\n",
    "\n",
    "        patron =['\\s\\s','\\s\"','\"\\s','\\s{']\n",
    "        replace = [' ','\"','\"','{']\n",
    "        data_v3 = data_v2\n",
    "        for i in range(len(patron)):\n",
    "            data_v3 = reg.sub(patron[i],replace[i],data_v3)\n",
    "\n",
    "        data_v4 = data_v3.replace(\",\",\";\")\n",
    "\n",
    "        data_v5 = data_v4.replace(\"\\\"\", \"\")\n",
    "\n",
    "        data_cleaned = data_v5.split(\"};\")\n",
    "\n",
    "        # String to List of dictionaries\n",
    "        diccionarios = []\n",
    "        for fila in data_cleaned:\n",
    "            keys = []\n",
    "            values = []\n",
    "            for pareja in fila[1:-1].split(';'):\n",
    "                elems =pareja.split(':')\n",
    "                keys.append(elems[0])\n",
    "                values.append(elems[1])\n",
    "            diccionarios.append(dict(zip(keys,values)))\n",
    "\n",
    "        # Schema for the new DF\n",
    "        data_schema = [StructField('idema',StringType(), False),\n",
    "                       StructField('lon', StringType(), True),\n",
    "                       StructField('fint', StringType(), True),\n",
    "                       StructField('prec', StringType(), True),\n",
    "                       StructField('alt', StringType(), True),\n",
    "                       StructField('vmax', StringType(), True),\n",
    "                       StructField('vv', StringType(), True),\n",
    "                       StructField('dv',StringType(), True), \n",
    "                       StructField('lat', StringType(), True),\n",
    "                       StructField('dmax', StringType(), True),\n",
    "                       StructField('ubi', StringType(), True),\n",
    "                       StructField('pres', StringType(), True),\n",
    "                       StructField('hr',StringType(), True), \n",
    "                       StructField('ts', StringType(), True),\n",
    "                       StructField('pres_nmar', StringType(), True),\n",
    "                       StructField('tamin', StringType(), True),\n",
    "                       StructField('ta', StringType(), True),\n",
    "                       StructField('tamax', StringType(), True),\n",
    "                       StructField('tpr', StringType(), True),\n",
    "                       StructField('vis', StringType(), True),\n",
    "                       StructField('stddv', StringType(), True),\n",
    "                       StructField('inso', StringType(), True),\n",
    "                       StructField('rviento', StringType(), True),\n",
    "                      ]\n",
    "        # Create and return the new DF\n",
    "        return self.spark.createDataFrame(diccionarios,schema = StructType(data_schema))\n",
    "    \n",
    "    def req_hoy_to_df(self,codigo):\n",
    "        print(\"CODIGO: \", codigo)\n",
    "        try:\n",
    "            api_response = self.api_observacion.datos_de_observacin__tiempo_actual_1(codigo)\n",
    "            pprint(api_response)\n",
    "        except ApiException as e:\n",
    "            pprint(api_response)\n",
    "            print(\"Exception: %s\\n\" % e)\n",
    "        r = requests.get(api_response.datos)\n",
    "        data = r.content\n",
    "        df_aemet = self.data_to_sparkdf(data)\n",
    "        print(\"OK\")\n",
    "\n",
    "        return df_aemet.select('idema','fint','prec','pres','tamax','tamin','dv','vv')\n",
    "    \n",
    "    def datos_aemet_hoy(self,codigos_estaciones):\n",
    "        lista_df =[]\n",
    "        for codigo in codigos_estaciones:\n",
    "            lista_df.append(self.req_hoy_to_df(codigo))\n",
    "        #Unimos\n",
    "        df = lista_df[0]\n",
    "        for i in range(1,len(lista_df)):\n",
    "            df = df.union(lista_df[i])\n",
    "        return df\n",
    "    \n",
    "    def media(self,vals):\n",
    "        validos = 0\n",
    "        nulos = 0\n",
    "        media = 0\n",
    "        for i in range(len(vals)):\n",
    "            if(vals[i] != 'None')or(vals[i] is None):\n",
    "                validos += 1\n",
    "                media += float(vals[i])\n",
    "            else:\n",
    "                nulos +=1\n",
    "        if(nulos == len(vals)):\n",
    "            return None\n",
    "        else:\n",
    "            return media/validos\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "        FUNCIONES PRINCIPALES <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    \n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName('clima_hoy').getOrCreate()\n",
    "        self.spark.sparkContext.setLogLevel('ERROR')\n",
    "        #Api AEMET\n",
    "        f = open(\"/home/rulicering/Datos_Proyecto_Ozono/Credenciales/Credenciales.json\")\n",
    "        credenciales = json.load(f)\n",
    "        AEMET_API_KEY = credenciales[\"aemet\"][\"api_key\"]\n",
    "        configuration = swagger_client.Configuration()\n",
    "        configuration.api_key['api_key'] = AEMET_API_KEY\n",
    "        self.api_observacion = swagger_client.ObservacionConvencionalApi(swagger_client.ApiClient(configuration))\n",
    "        \n",
    "    def process(self):\n",
    "        self.carga_estaciones()\n",
    "        self.aemet()\n",
    "        self.carga()\n",
    "        \n",
    "    def carga_estaciones(self):\n",
    "        ayer = (datetime.date.today() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "        df_estaciones = self.spark.read.csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Estaciones/Estaciones-\" + ayer +\".csv\",inferSchema= True, header= True)\n",
    "        self.df_estaciones = df_estaciones.cache()\n",
    "        \n",
    "        #Lista de magnitudes\n",
    "        regex = reg.compile(\"E_AEMET_HOY\")\n",
    "        self.c_magnitudes_aemet_hoy = [elem[-2:] for elem in list(filter(regex.search,df_estaciones.columns))]\n",
    "        \n",
    "    def aemet(self):\n",
    "        df_estaciones_aemet_hoy = self.df_estaciones.filter(self.df_estaciones[\"U_AEMET_HOY\"])\n",
    "        cod_estaciones_aemet_hoy = [elem[0] for elem in df_estaciones_aemet_hoy.select(\"CODIGO_CORTO\").collect()]\n",
    "        \n",
    "        df_aemet_hoy = self.datos_aemet_hoy(cod_estaciones_aemet_hoy)\n",
    "        df_aemet = df_aemet_hoy\n",
    "        \n",
    "        df_aemet = df_aemet.withColumn(\"ANO\",df_aemet[\"fint\"][0:4])\n",
    "        df_aemet = df_aemet.withColumn(\"MES\",df_aemet[\"fint\"][6:2])\n",
    "        df_aemet = df_aemet.withColumn(\"DIA\",df_aemet[\"fint\"][9:2])\n",
    "        df_aemet = df_aemet.withColumn(\"HORA\",df_aemet[\"fint\"][12:2])\n",
    "        df_aemet = df_aemet.withColumn(\"FECHA\",F.concat(df_aemet[\"fint\"][0:4],df_aemet[\"fint\"][6:2],df_aemet[\"fint\"][9:2]))\n",
    "        \n",
    "        pd_aemet = df_aemet.toPandas()\n",
    "        #Cambias comas por puntos\n",
    "        pd_aemet[\"tamax\"]  =  [reg.sub(',','.',str(x)) for x in pd_aemet[\"tamax\"]]\n",
    "        pd_aemet[\"tamin\"]  =  [reg.sub(',','.',str(x)) for x in pd_aemet[\"tamin\"]]\n",
    "        \n",
    "        pd_aemet[\"temp\"] = [self.media([pmax,pmin])for pmax,pmin in zip(pd_aemet[\"tamax\"].values, pd_aemet[\"tamin\"].values)]\n",
    "        \n",
    "        pd_aemet =pd_aemet.rename(columns={\"idema\":\"CODIGO_CORTO\",\n",
    "                                   \"vv\":\"81\",                         \n",
    "                                   \"dv\":\"82\",\n",
    "                                   \"temp\":\"83\",\n",
    "                                   \"pres\":\"87\",\n",
    "                                   \"prec\":\"89\",\n",
    "                                     })\n",
    "        \n",
    "        # Filtrar datos del dia que se lee     \n",
    "        ayer = '%02d' % (datetime.date.today()+datetime.timedelta(days=-1)).day\n",
    "        pd_aemet = pd_aemet[pd_aemet[\"DIA\"]== ayer]\n",
    "        \n",
    "        # Colocar columnas\n",
    "        columnas = [\"CODIGO_CORTO\",\"ANO\",\"MES\",\"DIA\",\"HORA\",\"FECHA\"]\n",
    "        for elem in self.c_magnitudes_aemet_hoy:\n",
    "            columnas.append(elem)\n",
    "        pd_aemet = pd_aemet[columnas]   \n",
    "            \n",
    "        # Tipos\n",
    "        for elem in self.c_magnitudes_aemet_hoy:\n",
    "            pd_aemet[elem]= pd_aemet[elem].astype(float)\n",
    "            \n",
    "        #Valores diarios\n",
    "        pd_aemet_media = pd_aemet.groupby(by=[\"CODIGO_CORTO\",\"ANO\",\"MES\",\"DIA\",\"FECHA\"]).agg({'81':'mean',\n",
    "                                                                         '82':'mean',\n",
    "                                                                         '83':'mean',\n",
    "                                                                         '87':'mean',\n",
    "                                                                         '89':'sum'})\n",
    "        #Nulos\n",
    "        pd_aemet_media = pd_aemet_media.replace(('None',None),np.nan)\n",
    "        self.pd_final =pd_aemet_media\n",
    "        \n",
    "    def carga(self):\n",
    "        pd_final = self.pd_final\n",
    "        #Los datos de ayer se cargan a las 3 am de hoy. 1 dia de diferencia\n",
    "        nuevo = (datetime.date.today() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "        anterior = (datetime.date.today() - datetime.timedelta(days=2)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        #BackUp\n",
    "        pd_final.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Clima/BackUp/Clima-\"+ nuevo + \".csv\")\n",
    "        pd_final.to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Clima/Clima-\"+ nuevo + \".csv\")\n",
    "        print(\"[INFO] - Clima-\"+ nuevo +\".csv --- Created successfully\")\n",
    "        \n",
    "        #Borrar la de ayer\n",
    "        try:\n",
    "            os.remove(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Clima/Clima-\"+ anterior + \".csv\")\n",
    "            print(\"[INFO] - Clima-\" +  anterior + \".csv --- Removed successfully\")\n",
    "        except:\n",
    "            print(\"[ERROR] - Clima-\" +  anterior + \".csv --- Could not been removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EJECUTAR\n",
    "clima_hoy = ClimaDia()\n",
    "clima_hoy.process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
