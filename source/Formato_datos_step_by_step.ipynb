{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formato_datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cogemos solamente los años 2019-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/rulicering/BigData/spark-2.4.5-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('punta_de_lanza').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incluimos datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contaminacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_2019 = spark.read.csv('/home/rulicering/Datos_Proyecto_Ozono/Raw/Contaminacion/datos_contaminacion_2019.csv',inferSchema= True,header=True, sep=';')\n",
    "df_2020 = spark.read.csv('/home/rulicering/Datos_Proyecto_Ozono/Raw/Contaminacion/datos_contaminacion_2020_03.csv',inferSchema= True,header=True, sep=';')\n",
    "df = df_2019.union(df_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PROVINCIA: integer (nullable = true)\n",
      " |-- MUNICIPIO: integer (nullable = true)\n",
      " |-- ESTACION: integer (nullable = true)\n",
      " |-- MAGNITUD: integer (nullable = true)\n",
      " |-- PUNTO_MUESTREO: string (nullable = true)\n",
      " |-- ANO: integer (nullable = true)\n",
      " |-- MES: integer (nullable = true)\n",
      " |-- D01: double (nullable = true)\n",
      " |-- V01: string (nullable = true)\n",
      " |-- D02: double (nullable = true)\n",
      " |-- V02: string (nullable = true)\n",
      " |-- D03: double (nullable = true)\n",
      " |-- V03: string (nullable = true)\n",
      " |-- D04: double (nullable = true)\n",
      " |-- V04: string (nullable = true)\n",
      " |-- D05: double (nullable = true)\n",
      " |-- V05: string (nullable = true)\n",
      " |-- D06: double (nullable = true)\n",
      " |-- V06: string (nullable = true)\n",
      " |-- D07: double (nullable = true)\n",
      " |-- V07: string (nullable = true)\n",
      " |-- D08: double (nullable = true)\n",
      " |-- V08: string (nullable = true)\n",
      " |-- D09: double (nullable = true)\n",
      " |-- V09: string (nullable = true)\n",
      " |-- D10: double (nullable = true)\n",
      " |-- V10: string (nullable = true)\n",
      " |-- D11: double (nullable = true)\n",
      " |-- V11: string (nullable = true)\n",
      " |-- D12: double (nullable = true)\n",
      " |-- V12: string (nullable = true)\n",
      " |-- D13: double (nullable = true)\n",
      " |-- V13: string (nullable = true)\n",
      " |-- D14: double (nullable = true)\n",
      " |-- V14: string (nullable = true)\n",
      " |-- D15: double (nullable = true)\n",
      " |-- V15: string (nullable = true)\n",
      " |-- D16: double (nullable = true)\n",
      " |-- V16: string (nullable = true)\n",
      " |-- D17: double (nullable = true)\n",
      " |-- V17: string (nullable = true)\n",
      " |-- D18: double (nullable = true)\n",
      " |-- V18: string (nullable = true)\n",
      " |-- D19: double (nullable = true)\n",
      " |-- V19: string (nullable = true)\n",
      " |-- D20: double (nullable = true)\n",
      " |-- V20: string (nullable = true)\n",
      " |-- D21: double (nullable = true)\n",
      " |-- V21: string (nullable = true)\n",
      " |-- D22: double (nullable = true)\n",
      " |-- V22: string (nullable = true)\n",
      " |-- D23: double (nullable = true)\n",
      " |-- V23: string (nullable = true)\n",
      " |-- D24: double (nullable = true)\n",
      " |-- V24: string (nullable = true)\n",
      " |-- D25: double (nullable = true)\n",
      " |-- V25: string (nullable = true)\n",
      " |-- D26: double (nullable = true)\n",
      " |-- V26: string (nullable = true)\n",
      " |-- D27: double (nullable = true)\n",
      " |-- V27: string (nullable = true)\n",
      " |-- D28: double (nullable = true)\n",
      " |-- V28: string (nullable = true)\n",
      " |-- D29: double (nullable = true)\n",
      " |-- V29: string (nullable = true)\n",
      " |-- D30: double (nullable = true)\n",
      " |-- V30: string (nullable = true)\n",
      " |-- D31: double (nullable = true)\n",
      " |-- V31: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formateo DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos contaminación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1] -  Quito columnas PROVICIA,MUNICIPIO,PUNTO_MUESTREO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v0 = df.select('ESTACION','MAGNITUD','ANO','MES','D01','V01','D02','V02','D03','V03','D04','V04','D05','V05','D06','V06','D07','V07','D08','V08','D09','V09','D10','V10','D11','V11','D12','V12','D13','V13','D14','V14','D15','V15','D16','V16','D17','V17','D18','V18','D19','V19','D20','V20','D21','V21','D22','V22','D23','V23','D24','V24','D25','V25','D26','V26','D27','V27','D28','V28','D29','V29','D30','V30','D31','V31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] - ESTACIÓN, MAGNITUD,ANO,MES,DIA,VALOR,VALIDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StringType,IntegerType,FloatType,StructType\n",
    "data_schema = [StructField('ESTACION',IntegerType(), False), \n",
    "              StructField('MAGNITUD',IntegerType(), False),\n",
    "              StructField('ANO',IntegerType(), False),\n",
    "              StructField('MES',IntegerType(), False),\n",
    "              StructField('VALOR',FloatType(), True),\n",
    "              StructField('VALIDO',IntegerType(), False),\n",
    "              StructField('DIA',IntegerType(), False)]\n",
    "struct = StructType(fields = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ESTACION: integer (nullable = false)\n",
      " |-- MAGNITUD: integer (nullable = false)\n",
      " |-- ANO: integer (nullable = false)\n",
      " |-- MES: integer (nullable = false)\n",
      " |-- VALOR: float (nullable = true)\n",
      " |-- VALIDO: integer (nullable = false)\n",
      " |-- DIA: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v1 = spark.createDataFrame(spark.sparkContext.emptyRDD(),struct)\n",
    "df_v1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "for i in range(1,32): #Días  \n",
    "    valor = 'D%02d' % i\n",
    "    valido = 'V%02d' % i\n",
    "    #df_v0.select(\"ESTACION\",\"MAGNITUD\",\"ANO\",\"MES\",valor,valido).printSchema()\n",
    "    df_v1 = df_v1.union(df_v0.select(\"ESTACION\",\"MAGNITUD\",\"ANO\",\"MES\",valor,valido).withColumn('DIA', lit(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorganizamos las columnas \n",
    "cols = df_v1.columns\n",
    "cols = cols[:4] + cols[-1:] + cols[-3:-1]\n",
    "df_v2= df_v1[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ESTACION=4, MAGNITUD=1, ANO=2019, MES=1, DIA=1, VALOR=18.0, VALIDO='V')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3] - StringIndexer sobre columna VALIDO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|VALIDO|\n",
      "+------+\n",
      "|     V|\n",
      "|     N|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v2.select(\"VALIDO\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringOrderType asignar indices (comenzando desde 0) en orden alfabetico Ascentende\n",
    "# En nuestro caso N va antes que la V (alfabéticamente) por lo que se asigna0 0->N, 1->V\n",
    "indexer = StringIndexer(inputCol =\"VALIDO\",outputCol = \"V\",stringOrderType=\"alphabetAsc\")\n",
    "df_v3 = indexer.fit(df_v2).transform(df_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|VALIDO|  V|\n",
      "+------+---+\n",
      "|     N|0.0|\n",
      "|     V|1.0|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v3.select(\"VALIDO\",\"V\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ESTACION', 'MAGNITUD', 'ANO', 'MES', 'DIA', 'VALOR', 'VALIDO', 'V']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v4 = df_v3.select('ESTACION', 'MAGNITUD', 'ANO', 'MES', 'DIA', 'VALOR','V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4] - Tratado de datos no validos VAL = 'N' a -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Utilizando esto no es necesario hacer el paso del string indexer.\n",
    "df_v5 = df_v4.withColumn(\"VALOR VALIDADO\",F.when(F.col(\"V\")== 0, -1).otherwise(F.col(\"VALOR\")) )\n",
    "#Nos quedamos con la columna VALOR VALIDADO y desechamos VALOR y VALIDADO\n",
    "df_v5 = df_v5.select(\"ESTACION\",\"MAGNITUD\",\"ANO\",\"MES\",\"DIA\",df_v5[\"VALOR VALIDADO\"].alias(\"VALOR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5] - ESTACION,ANO,MES,DIA,VM1,VM2,VM3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 14 magnitudes por lo que necesitamos 14 columnas nuevas\n"
     ]
    }
   ],
   "source": [
    "#Hay que ejecutar la parte [AUX] - Abajo\n",
    "print((\"Hay {0} magnitudes por lo que necesitamos {0} columnas nuevas\").format(df_M.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creamos la estructura <- NO UTILIZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema2 = [StructField('ESTACION',IntegerType(), False), \n",
    "              StructField('ANO',IntegerType(), False),\n",
    "              StructField('MES',IntegerType(), False),\n",
    "              StructField('DIA',IntegerType(), False),\n",
    "              StructField('VALOR',FloatType(), False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_magnitudes = df_M.select(\"MAGNITUD\").orderBy(\"MAGNITUD\",ascending=True).collect()\n",
    "for i in range(len(lista_magnitudes)):\n",
    "    #print((\"{0}:Magnitud {1}\").format(i+1,lista_magnitudes[i][0]))\n",
    "    elemento= lista_magnitudes[i][0]\n",
    "    nombre_columna = \"M_\"+f'{elemento:02}' \n",
    "    data_schema2.append(StructField(nombre_columna,FloatType(),True)) #El nulo por si acaso.\n",
    "data_schema2_1 = data_schema2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct2 = StructType(fields = data_schema2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v6 = df_v5.groupBy('ESTACION','ANO', 'MES', 'DIA').pivot(\"MAGNITUD\").sum(\"VALOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobar que los datos están bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---+---+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+\n",
      "|ESTACION|ANO |MES|DIA|1   |6   |7   |8   |9   |10  |12   |14  |20  |30  |35  |42  |43  |44  |\n",
      "+--------+----+---+---+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+\n",
      "|50      |2020|1  |1  |null|null|58.0|64.0|23.0|30.0|153.0|null|null|null|null|null|null|null|\n",
      "+--------+----+---+---+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v6.filter(df_v6[\"ANO\"]==2020).filter(df_v6[\"MES\"]==1).filter(df_v6[\"DIA\"]==1).filter(df_v6[\"ESTACION\"]==50).show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+---+---+-----+\n",
      "|ESTACION|MAGNITUD| ANO|MES|DIA|VALOR|\n",
      "+--------+--------+----+---+---+-----+\n",
      "|      50|       7|2020|  1|  1| 58.0|\n",
      "|      50|       8|2020|  1|  1| 64.0|\n",
      "|      50|       9|2020|  1|  1| 23.0|\n",
      "|      50|      10|2020|  1|  1| 30.0|\n",
      "|      50|      12|2020|  1|  1|153.0|\n",
      "+--------+--------+----+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v5.filter(df_v5[\"ANO\"]==2020).filter(df_v5[\"MES\"]==1).filter(df_v5[\"DIA\"]<=1).filter(df_v5[\"ESTACION\"]==50).orderBy(\"MAGNITUD\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_v6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5] - EXPORTARLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Más facil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.toPandas().to_csv(\"/home/rulicering/Datos_Proyecto_Ozono/Procesado/Contaminacion_2019_2020.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [AUX] -  MAGNITUDES & ESTACIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [!] - Faltas medidas de las magnitudes 37(Metaxileno), 38(Paraxileno) y 39(Ortoxileno)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº MAGNITUDES TOTAL: 14 -->17 - [37,38,39]\n",
      "Nº ESTACIONES TOTAL: 24\n"
     ]
    }
   ],
   "source": [
    "df_MxE =df_v0.select(\"MAGNITUD\",\"ESTACION\").distinct().orderBy(\"MAGNITUD\",\"ESTACION\") \n",
    "df_M = df_MxE.select(\"MAGNITUD\").distinct().orderBy(\"MAGNITUD\",ascending= False)\n",
    "df_E = df_MxE.select(\"ESTACION\").distinct().orderBy(\"ESTACION\",ascending= False)\n",
    "print(\"Nº MAGNITUDES TOTAL:\" ,df_M.count(), \"-->17 - [37,38,39]\")\n",
    "print(\"Nº ESTACIONES TOTAL:\" ,df_E.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de MAGNITUDES leidas por ESTACIÓN\n",
      "+--------+-----+\n",
      "|ESTACION|count|\n",
      "+--------+-----+\n",
      "|       4|    5|\n",
      "|       8|   14|\n",
      "|      11|    6|\n",
      "|      16|    5|\n",
      "|      17|    5|\n",
      "|      18|   10|\n",
      "|      24|   14|\n",
      "|      27|    4|\n",
      "|      35|    6|\n",
      "|      36|    6|\n",
      "|      38|    9|\n",
      "|      39|    5|\n",
      "|      40|    5|\n",
      "|      47|    5|\n",
      "|      48|    5|\n",
      "|      49|    4|\n",
      "|      50|    5|\n",
      "|      54|    4|\n",
      "|      55|   10|\n",
      "|      56|    7|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero de MAGNITUDES leidas por ESTACIÓN\")\n",
    "df_MxE.groupBy(\"ESTACION\").count().orderBy(\"Estacion\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|ESTACION|count|\n",
      "+--------+-----+\n",
      "|      27| 1860|\n",
      "|      47| 2325|\n",
      "|      16| 2325|\n",
      "|      40| 2325|\n",
      "|      57| 2790|\n",
      "|      54| 1860|\n",
      "|      48| 2325|\n",
      "|      17| 2325|\n",
      "|      35| 2790|\n",
      "|       4| 2325|\n",
      "|      55| 4650|\n",
      "|      59| 1860|\n",
      "|       8| 6510|\n",
      "|      39| 2325|\n",
      "|      49| 1860|\n",
      "|      50| 2325|\n",
      "|      38| 4185|\n",
      "|      24| 6510|\n",
      "|      60| 2325|\n",
      "|      56| 3255|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v2 = df_v1.groupBy(\"ESTACION\").count()\n",
    "df_v2.show()\n",
    "# df_v2 = df_v1.select(\"ESTACION\",df_v1[\"sum(MAGNITUD)\"].alias(\"Maginitudes\")).orderBy(\"ESTACION\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
